<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Olan Healy - FYP Showcase</title>

    <!-- Bootstrap CSS -->
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css"
      rel="stylesheet"
    />

    <!-- Bootstrap Icons -->
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css"
      rel="stylesheet"
    />

    <!-- Google Font -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap"
      rel="stylesheet"
    />

    <style>
      body {
        font-family: 'Inter', sans-serif;
        background: #0f172a;
        color: #e2e8f0;
        line-height: 1.6;
      }

      h1,
      h2,
      h3 {
        font-weight: 700;
      }

      .section-title {
        font-size: 2rem;
        margin-bottom: 1.5rem;
        display: flex;
        align-items: center;
        gap: 0.5rem;
      }

      .img-fluid rounded shadow {
        border-radius: 1rem;
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.4);
      }

      .rounded-box {
        background-color: #1e293b;
        border-radius: 1rem;
        padding: 2rem;
        transition: all 0.3s ease-in-out;
      }

      .rounded-box:hover {
        transform: scale(1.01);
      }

      .footer {
        color: #94a3b8;
        font-size: 0.9rem;
        padding: 2rem 0;
        text-align: center;
      }

      ul li {
        padding-bottom: 0.5rem;
      }

      .fade-in-up {
        opacity: 0;
        transform: translateY(20px);
        animation: fadeInUp 0.8s ease-out forwards;
      }

      @keyframes fadeInUp {
        to {
          opacity: 1;
          transform: translateY(0);
        }
      }
    </style>
  </head>

  <body>
    <div class="container py-5">
      <!-- Header -->
      <div class="text-center mb-5 fade-in-up">
        <h1 class="display-4 fw-bold">Context-Aware Bug Localisation</h1>
        <p class="lead">Using CodeBERT for Pre-Commit Java Project Analysis</p>
        <p class="text">Demo Day</p>
      </div>

      <section class="mb-5 fade-in-up">
        <h2 class="section-title"><i class="bi bi-flag-fill"></i> Introduction</h2>
        <div class="rounded-box">
          <p>
            This project investigates whether a <strong>context-aware, classification-based CodeBERT model</strong> can be used to localise pre-commit Java bugs using source-level inputs. It explores how contextual signals, such as surrounding code and commit messages can be used to identify buggy lines of code before they are introduced into a codebase.
          </p>
          <p>
            The research is guided by the question:<br>
            <em>Can a context-aware, classification-based CodeBERT model effectively localise pre-commit Java bugs using source-level inputs, and how does its performance compare to recent bug localisation approaches?</em>
          </p>
          <p>
            The main goals of the project are:
          </p>
          <ul>
            <li>To evaluate the effectiveness of CodeBERT for Java bug localisation</li>
            <li>To incorporate contextual features such as commit messages</li>
            <li>To compare results against recent bug localisation approaches</li>
            <li>To develop a prototype CLI tool for pre-commit bug detection</li>
          </ul>
        </div>
      </section>

<!-- Project Novelty -->
<section class="mb-5 fade-in-up">
  <h2 class="section-title"><i class="bi bi-stars"></i> Project Novelty</h2>
  <div class="rounded-box">
    <p>
      Compared to the models discussed in the <strong>literature review</strong>, this project introduces a novel approach to bug localisation by focusing on <strong>pre-commit detection</strong> using <em>source-level inputs</em> only, without requiring bug reports, runtime traces, or repair-based objectives.
    </p>
    <p>
      The model operates at the <strong>line level</strong> and uses static context available at commit time: the buggy code line, surrounding code, commit messages, and parent history. This enables localisation even before code reaches CI pipelines, making it uniquely suited for integration into real developer workflows.
    </p>
    <p>
      While other approaches use reinforcement learning, multi-line ranking, or weakly labelled supervision, this project leverages a <strong>frozen CodeBERT encoder</strong> with custom tokenisation (e.g. masked identifiers) and a classification head. The result is a lightweight, fast, and accurate bug localisation tool that can be easily integrated into existing development environments.
    </p>

    <div class="text-center mt-4">
      <img src="images/novelty.png" class="img-fluid rounded shadow" style="max-width: 800px; width: 100%;" alt="Project Novelty Diagram" />
      <p class="text-muted mt-2" style="font-size: 0.9rem;">
        Figure: Project novelty — focusing on pre-commit bug localisation using source-level features and commit context.
      </p>
    </div>
  </div>
</section>


<section class="mb-5 fade-in-up">
  <h2 class="section-title"><i class="bi bi-book-half"></i> Literature Review</h2>
  <div class="rounded-box">
    <p>
      I conducted a thorough review of recent academic studies and industry research to ground this project in the current state of bug localisation. The review traces the evolution of techniques, from traditional static analysis tools to advanced Large Language Models (LLMs) and directly informed my decision to fine-tune CodeBERT for pre-commit Java bug detection.
    </p>

    <h5 class="mt-4">Large Language Models (LLMs)</h5>
    <p>
      LLMs like BERT, GPT, and CodeBERT are built on the Transformer architecture introduced by Vaswani et al. These models process source code and natural language in parallel, capturing long-range dependencies and semantic meaning, essential for context-aware bug localisation. Their ability to integrate both code syntax and human-written commit messages makes them especially powerful in real-world developer tooling.
    </p>

    <h5 class="mt-4">CodeBERT</h5>
    <p>
      CodeBERT, developed by Microsoft, was pre-trained on millions of code–text pairs and supports six languages, including Java. It uses Masked Language Modelling (MLM) and Replaced Token Detection (RTD) as training objectives, enabling it to understand subtle code semantics and detect syntax or logic issues. Given its strong performance on tasks such as summarisation, retrieval, and repair, CodeBERT was chosen as the foundation for my classification model.
    </p>

    <h5 class="mt-4">Prior Work in Bug Localisation</h5>
    <ul>
      <li>
        <strong>Ciborowska & Damevski (2022):</strong> Used BERT with commit-level encodings to rank changesets. Their findings highlighted the value of including commit context, which I extended further via structured input fields.
      </li>
      <li>
        <strong>Mashhadi & Hemmati (2021):</strong> Applied CodeBERT in a repair setting using ManySStuBs4J. While effective on repeated bugs, they reported poor performance on unique examples, directly motivating my focus on generalisation.
      </li>
      <li>
        <strong>Zhang et al. (2024) – WELL:</strong> Proposed weak supervision to train localisation models using only binary labels. This inspired future directions around reducing labelling overhead while maintaining performance.
      </li>
      <li>
        <strong>RL-Locator (Chakraborty et al., 2024):</strong> Used reinforcement learning and bug reports to improve file-level localisation. While powerful, this approach is heavier and less applicable for pre-commit line-level scenarios like mine.
      </li>
    </ul>

    <h5 class="mt-4">Summary</h5>
    <p>
      While traditional tools like SonarQube rely on rule-based detection and often produce false positives, modern LLM-based methods leverage syntax, semantics, and commit context to improve accuracy. CodeBERT’s extensibility, dual-modality, and transformer-based architecture made it the ideal candidate for this project’s focus: <strong>pre-commit, source-level bug localisation</strong> in Java.
    </p>
  </div>
</section>

<!-- CLI Prototype -->
<section class="mb-5 fade-in-up">
  <h2 class="section-title"><i class="bi bi-terminal-fill"></i> CLI Prototype Integration</h2>
  <div class="rounded-box">
    <p>
      A working CLI prototype was developed to demonstrate how this bug localisation model could integrate into real-world software workflows. I can show you how it works here, with my demo. The tool intercepts <strong>staged diffs</strong>, builds a structured input sample, and returns a bug prediction <strong>before the code is committed</strong>.
    </p>

    <p>
      The system uses a <code>git commit-msg</code> hook and a Flask API backend, containerised with Docker for portability. When triggered, the hook sends the staged diff and commit message to the API, which loads the model and returns a binary prediction.
    </p>

    <p>
      It is important to note: <strong>this is a prototype</strong>. The tool has not yet been fully tested across large or diverse codebases.
    </p>

    <p>
      This prototype serves as a <em>proof of concept</em>, offering a glimpse into how the model might be embedded into developer workflows, such as Git hooks, CI pipelines, or pre-commit linters.
    </p>

    <div class="text-center mt-4">
      <img src="images/cli-demo.png" class="img-fluid rounded shadow" style="max-width: 700px; width: 100%;" alt="Model Architecture" />
      <p class="text-muted mt-2" style="font-size: 0.9rem;">Figure: CLI integration — Git hook intercepts staged commit and runs a pre-commit CodeBERT bug check.</p>
    </div>
  </div>
</section>
  
<!-- Methodology -->
<section class="mb-5 fade-in-up">
  <h2 class="section-title"><i class="bi bi-diagram-3-fill"></i> Methodology</h2>
  <div class="rounded-box">

    <p>
      The model was trained using the <strong>ManySStuBs4J</strong> dataset, which contains thousands of real bug-fix commits mined from open-source Java projects. Specifically, the <strong>repetition</strong> subset was used for training, validation, and testing as it includes the most examples. The <strong>unique</strong> subset was not used in this project because it is a strict deduplicated version of repetition, and thus does not provide additional examples.
    </p>

    <p>
      ManySStuBs4J consists of <strong>16 single-statement bug types</strong>, shown below. These patterns include identifier renaming, condition weakening/strengthening, operator changes, and exception handling edits. By using this dataset, the model learns to detect subtle bug patterns that commonly occur in codebases prior to commit.
    </p>

    <div class="text-center mt-4">
      <img src="images/bugtypes.png" class="img-fluid rounded shadow" alt="Bug Types from ManySStuBs4J" style="max-width: 700px; width: 100%;" />
      <p class="text-muted mt-2" style="font-size: 0.9rem;">Figure: The 16 bug types in ManySStuBs4J, representing common pre-commit mistakes.</p>
    </div>

    <h5 class="mt-4">Dataset Splitting</h5>
    <p>
      The dataset was split into training, validation, and test sets, with no overlap between examples. Each split retained the full structure of buggy and fixed lines, with balanced class distributions. Validation was used for early stopping, and the test set remained unseen until final evaluation.
    </p>

    <h5 class="mt-4">Input Formatting</h5>
    <p>
      Each training sample was preprocessed into a context-aware format with four input segments:
    </p>
    <ul>
      <li><code>[CONTEXT]</code> – Surrounding lines of code before</li>
      <li><code>[SNIPPET]</code> – The line suspected to be buggy</li>
      <li><code>[COMMIT]</code> – Commit message that fixed the bug</li>
      <li><code>[PARENT]</code> – Commit message of the parent version</li>
    </ul>
    <p>
      Variable and method names were <strong>masked with placeholders</strong> (e.g., <code>VAR1</code>, <code>FUNC1</code>) to improve generalisability across projects and reduce overfitting to specific codebases.
    </p>

    <div class="text-center mt-4">
      <img src="images/tokenisation.png" class="img-fluid rounded shadow" style="max-width: 700px; width: 100%;" alt="Tokenisation and Identifier Masking" />
      <p class="text-muted mt-2" style="font-size: 0.9rem;">
        Figure: Tokenisation and identifier masking applied to training inputs. Masking improves transferability across projects.
      </p>
    </div>

    <h5 class="mt-4">Final Preprocessing Pipeline</h5>
    <p>
      After preprocessing, all data was stored in JSON format with uniform structure across training, validation, and test sets. Each line was paired with a binary label and contained enriched context fields, ready to be tokenised by the model.
    </p>

    <div class="text-center mt-4">
      <img src="images/dataset.png" class="img-fluid rounded shadow" style="max-width: 700px; width: 100%;" alt="Final Preprocessed Dataset Structure" />
      <p class="text-muted mt-2" style="font-size: 0.9rem;">
        Figure: Final structured dataset used for model training and evaluation.
      </p>
    </div>

  </div>
</section>
  
  
<!-- Model & Training Pipeline -->
<section class="mb-5 fade-in-up">
  <h2 class="section-title"><i class="bi bi-cpu-fill"></i> Model & Training Pipeline</h2>
  <div class="rounded-box">
    <p>
      The model architecture is built on a <strong>frozen CodeBERT encoder</strong>, adapted for line-level Java bug classification. It was extended to support <strong>four custom tokens</strong> as shown above: <code>[CONTEXT]</code>, <code>[SNIPPET]</code>, <code>[COMMIT]</code>, and <code>[PARENT]</code>. These were injected into the tokenizer and embedding matrix, allowing the model to distinguish between structural elements of each input example.
    </p>

    <p>
      Freezing the encoder allowed the model to preserve the semantic representations learned during pre-training on code, while reducing the risk of overfitting on the relatively small fine-tuning dataset. Only the <strong>embedding layer</strong> and the <strong>classification head</strong> were left trainable, enabling adaptation to new tokens and the binary classification task.
    </p>

    <p>
      The classification head comprises two linear layers, separated by a <code>ReLU</code> activation and <code>Dropout</code> for regularisation. Predictions are made using the <code>[CLS]</code> token representation, with output indicating whether the target line is <em>Buggy</em> or <em>Not Buggy</em>.
    </p>

    <div class="text-center mt-4">
      <img src="images/codebert_pipeline.png" class="img-fluid rounded shadow" style="max-width: 600px; width: 100%;" alt="Model Architecture Diagram" />
      <p class="text-muted mt-2" style="font-size: 0.9rem;">
        Figure: The model ingests structured inputs, extracts the [CLS] embedding from a frozen CodeBERT encoder, and classifies lines using a lightweight prediction head.
      </p>
    </div>

    <p class="mt-4">
      A custom <code>MultiEvalTrainer</code> class was used to evaluate training and validation performance simultaneously, providing early insight into generalisation and allowing for better checkpoint selection. Validation was run on the validation split as mentnioned in the final preprocessing pipeline, while an additional <code>train_val</code> split of 500 random examples was sampled from the training set and used to determine early stopping.
    </p>

    <p>
      This strategy ensured that validation metrics weren’t biased by training history, and also helped prevent overfitting to a static validation set. The model was trained for up to 100,000 episodes with <strong>early stopping triggered by the highest F1 score</strong> on the <code>train_val</code> set, rather than just monitoring loss.
    </p>

    <p>
      Training used a linear learning rate scheduler with decay from <code>2e-5</code> to near-zero, with logs recorded every 1000 steps. These measures contributed to stable convergence, generalisable performance, and a lightweight model suitable for real-world integration.
    </p>
  </div>
</section>

  
<!-- Training Progress -->
<section class="mb-5 fade-in-up">
  <h2 class="section-title"><i class="bi bi-graph-up"></i> Training Progress</h2>
  <div class="rounded-box">
    <p>
      The model was trained for approximately 100,000 steps with metrics tracked every 1,000 steps. The dashboard below visualises this full trajectory — including training loss, learning rate decay, evaluation performance, and F1 vs accuracy trends across both datasets.
    </p>

    <p>
      The top-left graph shows a steady decline in <strong>training loss</strong>, with no signs of instability. Evaluation loss (top-right) also improved across both validation and <code>train_val</code> splits, confirming stable generalisation. The learning rate graph (top-centre) shows the expected linear decay strategy from <code>2e-5</code> to near-zero.
    </p>

    <p>
      Mid-row plots reveal a consistent improvement in <strong>accuracy and F1</strong>, which plateaued around 80% on both splits. Importantly, <strong>validation metrics closely tracked training</strong>, showing no divergence or overfitting.
    </p>

    <p>
      Bottom-row plots compare <strong>precision and recall</strong>. The model exhibited a deliberate lean towards recall, catching most buggy lines, even if it meant allowing some false positives. This behaviour is ideal for pre-commit tools where missing bugs is costlier than flagging extra lines.
    </p>

    <p>
      Early stopping was guided by F1 on the <code>train_val</code> subset, not validation loss, allowing for more robust checkpoint selection. Overall, the model demonstrated strong learning dynamics, balanced bias-variance trade-off, and a stable convergence curve across all metrics.
    </p>

    <img src="images/full_training_dashboard.png" class="img-fluid rounded shadow mt-4" alt="Training Dashboard" />
  </div>
</section>

<section class="mb-5 fade-in-up">
  <h2 class="section-title"><i class="bi bi-bar-chart-fill"></i> Results and Discussion</h2>
  <div class="rounded-box">

    <h5 class="mt-4">In-Distribution Performance</h5>
    <p>
      The model was evaluated on a held-out <strong>test split</strong>, containing examples that were <em>not seen during training</em>, but drawn from the same overall distribution. This provided a fair measure of the model’s ability to detect familiar bug types without overfitting to specific instances.
    </p>
    
    <ul>
      <li><strong>Accuracy:</strong> 0.78</li>
      <li><strong>F1 Score:</strong> 0.80</li>
      <li><strong>Precision:</strong> 0.73</li>
      <li><strong>Recall:</strong> 0.87</li>
    </ul>
    <p>
      As seen in the confusion matrix, the model favours high recall, prioritising the detection of buggy lines even at the cost of some false positives — a desirable trait in pre-commit tools. The Visualisations earlier of the training metrics confirm this behaviour, showing stable convergence and consistent validation trends.
    </p>
    <div class="row g-4 mt-4">
      <div class="col-md-6">
        <img src="images/metrics_repetition.png" class="img-fluid rounded shadow" alt="Repetition Metrics" />
      </div>
      <div class="col-md-6">
        <img src="images/confusion_matrix_repetition.png" class="img-fluid rounded shadow" alt="Repetition Confusion Matrix" />
      </div>
    </div>

    <h5 class="mt-5">Cross-Dataset Generalisation (Defects4J)</h5>
    <p>
      To test generalisation beyond ManySStuBs4J, the final model was evaluated on <strong>30 single-line Java bugs</strong> extracted from the <strong>Defects4J</strong> benchmark. These examples were manually reformatted to fit the model’s input structure using <code>[CONTEXT]</code>, <code>[SNIPPET]</code>, <code>[COMMIT]</code>, and <code>[PARENT]</code> fields. All identifiers were masked to match the training regime.
    </p>
    <p>
      Without any additional fine-tuning, the model achieved the following results:
    </p>
    <ul>
      <li><strong>Top-1 Accuracy:</strong> 62%</li>
      <li><strong>F1 Score:</strong> 0.76</li>
    </ul>
    <p>
      These results demonstrate the model’s ability to generalise to unseen, real-world bug fixes. The line-level classification setup ensures Top-1 Accuracy is a directly meaningful and valid metric. Despite differences in how other studies may process Defects4J, these results clearly support the feasibility of this approach for practical pre-commit bug detection.
    </p>
    <div class="row g-4 mt-4">
      <div class="col-md-6">
        <img src="images/metrics_defects4j.png" class="img-fluid rounded shadow" alt="Defects4J Metrics" />
      </div>
      <div class="col-md-6">
        <img src="images/confusion_matrix_defects4j.png" class="img-fluid rounded shadow" alt="Defects4J Confusion Matrix" />
      </div>
    </div>
    <p class="mt-4">
      Ranking metrics such as <strong>MRR</strong> and <strong>MAP</strong> were not computed, as my model performs <em>binary classification</em> on individual lines. These metrics are only relevant in ranked retrieval settings, which my system does not implement.
    </p>

    <h5 class="mt-5">Comparative Analysis with Prior Studies</h5>
    <p>
      To evaluate how this context-aware classification model compares with recent bug localisation research, results were benchmarked against four representative studies:
    </p>
    <div class="text-center my-4">
      <img src="images/table_comparison.png" class="img-fluid rounded shadow" style="max-width: 700px; width: 100%;" alt="Comparison Table" />
      <p class="text-muted mt-2" style="font-size: 0.9rem;">Figure: Accuracy comparison across different bug localisation models and datasets.</p>
    </div>
    <ul>
      <li>
        <strong>Mashhadi & Hemmati (2021):</strong> Applied CodeBERT in a <em>sequence-to-sequence repair</em> setup. Their model achieved 72% Top-1 Accuracy on the ManySStuBs4J repetition subset and just 15% on Defects4J. In contrast, my project’s <strong>classification-based approach</strong> reached 78% and 62% respectively, demonstrating that even without generation or repair, structured classification with commit context can outperform repair-based models in bug localisation tasks.
      </li>
    
      <li>
        <strong>WELL (Zhang et al., 2023):</strong> Used weak supervision to train localisation models, reporting 29% Top-1 Accuracy on datasets like StuBug and VarMisuse. While WELL avoids manual labelling, its performance is substantially below this model’s 78% (repetition) and 62% (Defects4J), highlighting the advantage of combining structured code with commit metadata.
      </li>
    
      <li>
        <strong>RL-Locator (Chakraborty et al., 2024):</strong> Introduced reinforcement learning and bug reports for file-level localisation. Although RL-Locator achieved 46% Top-1 Accuracy with MRR 0.50 and MAP 0.47, its setup relies on post-commit bug reports and search over candidate lines. By contrast, this project offers a <strong>lightweight pre-commit solution</strong> requiring no bug reports or ranked output.
      </li>
    
      <li>
        <strong>Ciborowska & Damevski (2022):</strong> Used BERT to rank changesets based on bug reports, reporting MRR of 0.355 and MAP of 0.31. Their task differs from this project’s line-level classification goal, but both share a focus on commit context. This project instead targets direct <em>per-line</em> detection without the need for ranking mechanisms or external bug reports.
      </li>
    </ul>
    
    <p>
      Overall, this comparison shows that structured classification with source-level context and commit history can match or exceed more complex techniques like weak supervision, ranking, and repair. While not directly comparable in task formulation, the higher Top-1 Accuracy demonstrates strong bug localisation capability in a pre-commit setting.
    </p>

    <h5 class="mt-5">Discussion – Answering the Research Question</h5>
    <p>
      This project set out to investigate the research question: <em>Can a context-aware, classification-based CodeBERT model effectively localise pre-commit Java bugs using source-level inputs, and how does its performance compare to recent localisation approaches?</em>
    </p>
    <p>
      The experimental results presented above provide strong support for this. The model consistently localises buggy lines with high performance on both in-distribution and cross-project data, demonstrating that structured context and a classification head are sufficient to compete with more complex generative or ranking-based systems.
    </p>
    <p>
      By incorporating structured input fields like <code>[CONTEXT]</code>, <code>[SNIPPET]</code>, <code>[COMMIT]</code>, and <code>[PARENT]</code>, the model captures richer signals beyond syntax, such as semantic history and developer intent, allowing more accurate localisation.
    </p>
    <p>
      These results confirm that a CodeBERT-based model, trained solely on pre-commit Java code and enhanced with contextual features, can serve as a practical and effective bug localisation tool. It meets both the technical objectives (accuracy, recall) and the broader research aim of designing generalisable and lightweight pre-commit developer support tools.
    </p>
    <h5 class="mt-5">Threats to Validity</h5>
    <ul>
      <li><strong>Defects4J Filtering:</strong> Only single-line bug fixes were evaluated in Defects4J, with no non-buggy lines included. This inflates precision and limits F1 interpretability.</li>
      <li><strong>Metric Scope:</strong> The model is evaluated using binary classification metrics. MAP and MRR were not used, which limits comparability with ranking-based systems.</li>
      <li><strong>Context Truncation:</strong> Inputs are truncated at 512 tokens. Long methods or verbose commit messages may lead to the loss of key context features.</li>
      <li><strong>Frozen Encoder Limitations:</strong> Freezing CodeBERT improved stability, but limited adaptability. Fully unfreezing led to overconfident predictions, so partial fine-tuning remains an open avenue.</li>
      <li><strong>Synthetic Dataset Constraints:</strong> ManySStuBs4J is synthetic and only covers 16 bug types. While useful for benchmarking, it lacks the noise, ambiguity, and variety found in natural bug data.</li>
    </ul>
    
    <h5 class="mt-5">Future Work</h5>
    <ul>
      <li><strong>Selective Fine-tuning:</strong> Explore partial unfreezing or adapter modules to balance generalisability and adaptability, especially for subtle or less frequent bugs.</li>
      <li><strong>Multi-Line and Multi-Hunk Support:</strong> Extend input structure to support multi-line diffs and multi-hunk commits using hierarchical encodings or attention layers.</li>
      <li><strong>Balanced Evaluation Sets:</strong> Introduce both buggy and clean lines in test sets to better compute precision, recall, and ranking-based metrics.</li>
      <li><strong>Confidence-Based Ranking:</strong> Modify classifier outputs into ranked line lists using softmax scores, enabling fair comparison with retrieval systems via MAP/MRR.</li>
      <li><strong>Cross-Language Generalisation:</strong> Reapply the pipeline to languages like Python using multilingual CodeBERT, and assess transfer across syntax domains.</li>
    </ul>

    <p>
      In conclusion, this study presents a compelling proof-of-concept for context-aware, classification-based bug localisation using CodeBERT. The system achieved strong results, generalised well to new data, and offers a scalable approach for improving software quality at the pre-commit stage.
    </p>
  </div>
</section>






      <!-- Footer -->
      <footer class="footer fade-in-up">
        Olan Healy — 21318204 <br />
        BSc Computer Systems — LM051
      </footer>
    </div>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"></script>
  </body>
</html>
